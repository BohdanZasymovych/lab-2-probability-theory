---
title: 'P&S-2025: Lab assignment 2'
author: "Viktoriya Kibyeryeva, Solomiia Gadiichuk, Sofiia Pereima"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

### Work Breakdown

-   **Viktoriya Kibyeryeva** : Implemented Task 2 and Task 4 (Subtasks
    1.1 and 1.2)

-   **Solomiia Gadiichuk** : Implemented Task 3 and Task 4 (Subtasks 2.2
    and 2.3)

-   **Sofiia Pereima**: Implemented Task 1 and Task 4 (Subtasks 1.3 and
    2.1)

## General comments and instructions

-   Complete solution will give you **4 points** (working code with
    explanations + oral defense). Submission deadline **November 1,
    2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to
    **cms** both the source *R notebook* **and** the generated html
    file\
-   At the beginning of the notebook, provide a work-breakdown structure
    estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer
        to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is
        just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you
        use to complete the task) as well as histograms etc to
        illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding
        theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree
        with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit**
    ordinal number of your team on the list. Include the line
    **set.seed(team id number)** at the beginning of your code to make
    your calculations reproducible. Also observe that the answers **do**
    depend on this number!\
-   Take into account that not complying with these instructions may
    result in point deduction regardless of whether or not your
    implementation is correct.

------------------------------------------------------------------------

```{r}
set.seed(6)
```

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it
    to a $7$-bit *codeword*
    $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where
    $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the
    received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome
    vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$
    *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary
    $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no.
    $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or
    more than one), while $(1 1 0 )$ means the third bit (or more than
    one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in
    $\mathbf{r}$ to get the corrected
    $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and
    find the estimate $\hat p$ of the probability $p^*$ of correct
    transmission of a single message $\mathbf{m}$. Comment why, for
    large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator
    of success by the standard error of your sample and using the CLT,
    predict the \emph{confidence} interval
    $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate
    $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while
    transmitting a $4$-digit binary message. Do you think it is one of
    the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
# your team id number 
                          ###
id <- 6                ### Change to the correct id!
                          ###
#set.seed(id)
p <- id/100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
# cat("The matrix G is: \n") 
#G  
#cat("The matrix H is: \n") 
#H
#cat("The product GH must be zero: \n")
#(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages
N<-100
message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(100)
codewords <- (messages %*% G) %% 2
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
#  errors <- ...
errors_generator <- function(N, p) {
  matrix(sample(c(0, 1), 7 * N, replace = TRUE, prob = c(1 - p, p)), nrow = N)
}
#  received <-   
errors <- errors_generator(100, p)
received_messages <- (codewords + errors) %%2
# Detect the errors (multiply on H)
syndrome <- received_messages %*% H %% 2
```

The next steps include detecting the errors in the received messages,
correcting them, and then decoding the obtained messages. After this,
you can continue with calculating all the quantities of interest

```{r}
successes <- 0
err_num <- numeric(5)

# Correct the errors
for (i in 1:100) {
  codewords_main <- codewords[, c(3, 5, 6, 7)]
  received_main <- received_messages[, c(3, 5, 6, 7)]
  count = sum(codewords_main[i, ] != received_main[i, ])
  err_num[count + 1] <- err_num[count + 1] + 1
  if (all(syndrome[i, ] == c(0,0,0))) {
    successes <- successes+1
  } else {
    ind <- sum(syndrome[i, ] * c(1, 2, 4))
    received_messages[i, ind] <- 1-received_messages[i, ind]
    if (received_messages[i, 3] == codewords[i, 3] && received_messages[i, 5] == codewords[i, 5] && received_messages[i, 7] == codewords[i, 7] && received_messages[i, 6] == codewords[i, 6]) {
      successes <- successes + 1
    }
  }
  
}
cat("Number of succeses:", successes, "\n")
cat("Number of errors 0,1,2,3,4: ", err_num, "\n")
```

```{r}
#practical value of probabilty, which we will later compare with the actual 
hat_p <- successes/100
hat_p
```

```{r}
# Calculate the standard deviation of the estimate
se <- sqrt(hat_p * (1 - hat_p) / N)

# Calculate the half-length of the confidence interval
epsilon <- 1.96 * se
p_star <- 0.329417

cat("we will have epsilon: ", epsilon, "\n")
cat("intervals will be: (", p_star-epsilon, ",", p_star+epsilon, ")\n")

# Calculate the required sample size to guarantee that epsilon <= 0.03
N_required <- 1.96^2 * p_star * (1-p_star) / 0.03^2

cat("N required: ", ceiling(N_required))


x_values <- 0:4
barplot(err_num, names.arg = x_values, xlab = "k = 0,1,2,3,4", ylab = "Frequency", main = "Histogram of errors in codewords in information bits for N = 100", col = "skyblue")
```

**Do not forget to include several sentences summarizing your work and
the conclusions you have made!**

In the first task, we need to find theoretical probability and for this
task we use the following formula(we count the correct ones, when 1 bit
is faulty):

$$
p^{*} = (1-p)^7 + C_7^1(1-p)^6p = 0.7^7 + 7*0.7^6*0.3 = 0.329417...
$$

In the second task we need to find the interval for our p_hat(practical
probability - probability, which is derived by r program) to be bigger
or equal then 0,95.

Formulas, which we use for our calculations, are th following:

$$
P(p^* - \varepsilon <= \hat p <= p^* + \varepsilon)
$$

$$
Z_n = \sqrt n * (\hat p - p^*)/σ = \sqrt n * (\hat p - p^*) / \sqrt {p^* (1-p^*)}
$$

$$
P(|\hat p - p^*| >= \varepsilon) = P(|Z_n| >= \varepsilon \sqrt n / \sqrt {p^*(1-p^*}) = 2P(Z_n <= - \sqrt n \varepsilon / \sqrt {p^*(1-p^*)}) ≈ 2Ф(Z_n <= - \sqrt n \varepsilon / \sqrt {p^* (1-p^*)})
$$

The meaning of cdf at the concrete point is constant, therefore we can
look it up in the Internet and insert in our formula:

$$
\sqrt n \varepsilon / \sqrt {p^* (1-p^*)} >= 1.96
$$

$$
\varepsilon >= 1.96 * \sqrt {p^*(1-p^*)}/\sqrt n = 1.96 * 0.470002 / \sqrt n = 0.921203 / \sqrt n
$$

In task 3 we have to find the minimal quantity of messages to have
$$\varepsilon$$ less or equal to 0,03 :

$$
\sqrt n \varepsilon / \sqrt {p^* (1-p^*)} >= 1.96
$$

$$
\sqrt n >= 1.96 * \sqrt {p^*(1-p^*)} / \varepsilon
$$

$$
n >= 1.96^2 * p^*(1-p^*) / \varepsilon ^2 = 1.96^2 * 0.329417*(1-0.329417) / 0.03^2 = 942,90552
$$

So our minimal quantity of messages should be approximately 943.

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a
Poisson distribution. As you remember, a Poisson random variable
describes occurrences of rare events, i.e., counts the number of
successes in a large number of independent random experiments. One of
the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big
*half-life period* $T$; it is vitally important to know the probability
that during a one second period, the number of nuclei decays will not
exceed some critical level $k$. This probability can easily be estimated
using the fact that, given the *activity* ${\lambda}$ of the element
(i.e., the probability that exactly one nucleus decays in one second)
and the number $N$ of atoms in the sample, the random number of decays
within a second is well modelled by Poisson distribution with parameter
$\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms
is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro
constant, and $M$ is the molar (atomic) mass of the element. The
activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is
measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive
element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life
period $T = 30.1$ years and mass
$m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by
$X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays
in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll
    need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$
    gets very close to a normal one as $n$ becomes large and identify
    that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and
        calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical
        cumulative distribution function $\hat F_{\mathbf{s}}$ of
        $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$
        of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.}
        $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to
        visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two
        \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the
        results.\
3.  Calculate the largest possible value of $n$, for which the total
    number of decays in one second is less than $8 \times 10^8$ with
    probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality,
        Chernoff bound and Central Limit Theorem, and compare the
        results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and
        calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less
        than critical value ($8 \times 10^8$) and calculate the
        empirical probability; comment whether it is close to the
        desired level $0.95$

```{r}
# Parameters
lambda <- log(2)/(30.1*365*24*60*60)
m <- 6 * 10^(-6)
Na <- 6 * 10^23
mmol <- 136.907
N <- m * Na/mmol
mu <- N * lambda
K <- 1e3
n <- 5
#Calculating sample means
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
# Approximating Normal distribution parameters
mu <- mu
sigma <- sqrt(mu/n)
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

```{r}
K <- 1000
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
Fs <- ecdf(sample_means)
#Plotting cdf and ecdf for n =5
xlims <- c(mu-3*sigma,mu+3*sigma)
plot(Fs, 
     xlim = xlims, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf(n=5)")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)


```

```{r}

#Calculating maximal difference for n = 5
Fs <- ecdf(sample_means)

t <- seq(mu - 3*sigma, mu + 3*sigma, length.out = 200)

Fs_values <- Fs(t)
Fz_values <- pnorm(t, mean = mu, sd = sigma)

max_diff <- max(abs(Fs_values - Fz_values))

cat("Maximal difference:", max_diff, "\n")

```

```{r}
# Plotting cdf and ecdf for n = 10
n <- 10
K <- 1000
mu <- N * lambda
sigma <- sqrt(mu/n)
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
Fs <- ecdf(sample_means)

xlims <- c(mu-3*sigma,mu+3*sigma)
plot(Fs, 
     xlim = xlims, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf(n=10)")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)


```

```{r}
#Calculating maximal difference for n = 10
Fs <- ecdf(sample_means)

t <- seq(mu - 3*sigma, mu + 3*sigma, length.out = 200)

Fs_values <- Fs(t)
Fz_values <- pnorm(t, mean = mu, sd = sigma)

max_diff <- max(abs(Fs_values - Fz_values))

cat("Maximal difference:", max_diff, "\n")
```

```{r}
# Plotting cdf and ecdf for n = 50
n <- 50
K <- 1000
mu <- N * lambda
sigma <- sqrt(mu/n)
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
Fs <- ecdf(sample_means)

xlims <- c(mu-3*sigma,mu+3*sigma)
plot(Fs, 
     xlim = xlims, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf(n = 50)")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)


```

```{r}
#Calculating maximal difference for n = 50
Fs <- ecdf(sample_means)

t <- seq(mu - 3*sigma, mu + 3*sigma, length.out = 200)

Fs_values <- Fs(t)
Fz_values <- pnorm(t, mean = mu, sd = sigma)

max_diff <- max(abs(Fs_values - Fz_values))

cat("Maximal difference:", max_diff, "\n")

```

```{r}
#Calculating Markov bound
a <- 8e8
n_markov_bound <- (0.05 * a) / mu
n_markov_max <- floor(n_markov_bound)
cat("Markov: n < ", n_markov_bound, "; largest integer n =", n_markov_max, "\n")

```

```{r}
#Calculating Chernoff bound
chernoff_bound_for_lambda <- function(lam) {
  # Exponent of the bound
  h <- function(t) (-t * a + lam * (exp(t) - 1))
  # Minimizing the exponent
  opt <- optimize(h, interval = c(1e-12, 10))
  min_exp <- opt$objective
  # Calculating the bound
  bound <- exp(min_exp)
  return(bound)
}
# Setting bounds
max_n_try <- 200
cher_max_n <- NA
cher_bounds <- numeric(max_n_try)
# Iterating until we find the bound or until the number of trials is exceeded
for (n in 1:max_n_try) {
  lam_n <- n * mu
  b <- chernoff_bound_for_lambda(lam_n)
  cher_bounds[n] <- b
  if (b < 0.05) cher_max_n <- n
}

cat("Chernoff: largest n with bound < 0.05  = ", cher_max_n, "\n")

```

```{r}
Phi <- function(z) pnorm(z)
target <- 0.95
# Helper function
gfun <- function(lam) {
  z <- (a - lam) / sqrt(lam)
  return(pnorm(z) - target)
}
# Setting bounds
lo <- a * 0.9
hi <- a * 0.9999999
if (gfun(lo) < 0) {
  lo <- a * 0.5
}
# Finding lambda
res_clt <- uniroot(gfun, lower = lo, upper = hi, tol = 1e-12, maxiter = 200)
lambda_clt <- res_clt$root
# Convering lambda to n
n_clt_exact <- lambda_clt / mu
n_clt_max <- floor(n_clt_exact)
cat(sprintf("CLT: lambda ~= %.6e, n ~= %.6f -> largest integer n = %d\n",
            lambda_clt, n_clt_exact, n_clt_max))

```

```{r}
# Setting upper bound
upper_bound <- (ceiling(a / mu) + 10)
found_n_exact <- NA
#Iterating until we find the bound or until we reach upper bound
for (n in 1:upper_bound) {
  lam_n <- n * mu
  # Calculating probability
  p_less <- ppois(a - 1, lambda = lam_n)
  # Checking if it satisfies the condition
  if (p_less > 0.95) {
    found_n_exact <- n
  } else {
    if (!is.na(found_n_exact)) break
  }
}
if (is.na(found_n_exact)) {
  cat("No n <= ", upper_bound, " satisfied P(S_n < a) > ", 0.95, "\n", sep = "")
} else {
  cat("Poisson CDF: largest integer n with P(S_n < a) > ", 0.95, " is n = ", found_n_exact, "\n", sep = "")
}
```

In part $2$, we compared the distribution of the sample means of
$X_1, X_2, ...$ to Normal distribution. Even though theoretically
maximal difference should get smaller as $n$ increase, in our case it is
the smallest for $n = 5$. This is so because because we have finite
number of simulated samples and the grid is discrete.

In part three, we calculated the largest $n$ for which the total number
of decays in $1$ second is less than $8 \times 10^8$ with probability
greater than $0.95$. The Central Limit Theorem and Chernoff bound gave
bounds equal to the one obtained from simulation, which is $41$. The
Markov theorem provided a much weaker bound, equal to $2$, because it
only takes into account the mean, not the variance or the distribution
shape.

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of
    $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as
    $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the
        \textbf{r.v.} $X_i$ and calculate the sample mean
        $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the
        \emph{empirical cumulative distribution} function
        $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of
        $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.}
        $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph
        to visualize their proximity;\
    -   calculate the maximal difference between the two
        \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the
        results.
2.  The place can be considered safe when the number of clicks in one
    minute does not exceed $100$. It is known that the parameter $\nu$
    of the resulting exponential distribution is proportional to the
    number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where
    $\nu_1$ is the parameter for one sample. Determine the maximal
    number of radioactive samples that can be stored in that place so
    that, with probability $0.95$, the place is identified as safe. To
    do this,
    -   express the event of interest in terms of the \textbf{r.v.}
        $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov
        inequality,Chernoff bound and Central Limit Theorem and compare
        the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization
        $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum
        $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample
        $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the
        $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe
        and compare to the desired level $0.95$

#### First, generate samples an sample means:

```{r}
nu1 <- id + 10
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rexp(n*K, rate = 1/nu1), nrow=n))
t <- seq(min(sample_means), max(sample_means), length.out = 400)
```

#### We know each $X_i \sim \text{Exp}(\nu_1)$, so:

-   The mean of one $X_i$ is $\mathbb{E}[X_i] = \frac{1}{\nu_1}$

-   The variance of one $X_i$ is $\text{Var}(X_i) = \frac{1}{\nu_1^2}$

Now, since $\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$ is the sample
mean:

-   $\mathbb{E}[\bar{X}_n] = \frac{1}{n} \cdot n \cdot \frac{1}{\nu_1} = \frac{1}{\nu_1}$

-   $\text{Var}(\bar{X}_n) = \frac{1}{n^2} \cdot n \cdot \frac{1}{\nu_1^2} = \frac{1}{n \nu_1^2}$

So we use: $$
\mu = \frac{1}{\nu_1}, \quad \sigma^2 = \frac{1}{n \nu_1^2}
$$

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu    <- nu1
sigma <- nu1 / sqrt(n)


prac_cdf <- ecdf(sample_means) # Empirical cdf

Fs <- ecdf(sample_means)
t  <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 400)
Fz <- pnorm(t, mean = mu, sd = sigma)   # Theoretical CDF
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)

plot(Fs,
     xlim = xlims,
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
lines(t, Fz, col = "red", lwd = 2)





```

```{r}

max_diff <- max(abs(Fs(t) - Fz))
max_diff

```

For n = 10

```{r}

n <- 10
sample_means <- colMeans(matrix(rexp(n*K, rate = 1/nu1), nrow=n))
t <- seq(min(sample_means), max(sample_means), length.out = 400)


mu    <- nu1
sigma <- nu1 / sqrt(n)


prac_cdf <- ecdf(sample_means)

Fs <- ecdf(sample_means)
t  <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 400)
Fz <- pnorm(t, mean = mu, sd = sigma)


xlims <- c(mu-3*sigma,mu+3*sigma)

plot(Fs,
     xlim = xlims,
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
lines(t, Fz, col = "red", lwd = 2)





```

```{r}

max_diff <- max(abs(Fs(t) - Fz))
max_diff

```

for n = 50

```{r}


n <- 50
sample_means <- colMeans(matrix(rexp(n*K, rate = 1/nu1), nrow=n))
t <- seq(min(sample_means), max(sample_means), length.out = 400)


mu    <- nu1
sigma <- nu1 / sqrt(n)


prac_cdf <- ecdf(sample_means)

Fs <- ecdf(sample_means)
t  <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 400)
Fz <- pnorm(t, mean = mu, sd = sigma)


xlims <- c(mu-3*sigma,mu+3*sigma)

plot(Fs,
     xlim = xlims,
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
lines(t, Fz, col = "red", lwd = 2)




```

```{r}

max_diff <- max(abs(Fs(t) - Fz))
max_diff

```

### **Conclusions**

The three plots illustrate how the distribution of the sample means of
the exponential random variables\
$$
X_1, X_2, \dots, X_n \sim \mathrm{Exp}(\nu_1), \qquad \nu_1 = 16,
$$ approaches the normal distribution as the sample size $n$ increases.

-   For $n = 5$, the empirical CDF (blue) is visibly different from the
    theoretical normal CDF (red).\
    The maximal difference between the two distributions is\
    $$
    D_{n=5} =  0.06123401
    $$ This means that the sample means still have noticeable skewness
    from the exponential shape.

-   For $n = 10$, the curves are already much closer, and the difference
    decreases to\
    $$
    D_{n=10} = 0.03774941.
    $$ The sample means begin to follow a nearly symmetric, bell-shaped
    pattern.

-   For $n = 50$, the empirical and normal CDFs almost coincide, with\
    $$
    D_{n=50} = 0.02983102
    $$ The normal approximation is now very accurate.

These results clearly confirm the **Central Limit Theorem (CLT)**:\
as the sample size $n$ increases, the distribution of the sample mean\
$\bar{X}_n$ becomes approximately normal with\
$$
\bar{X}_n \sim \mathcal{N}\!\left(\mu = \nu_1, \ \sigma^2 = \frac{\nu_1^2}{n}\right).
$$

2)  

### Modeling the Safety Condition

We’re analyzing a radioactive storage setup where the time between
clicks of a Geiger counter follows an exponential distribution:

$$
X_i \sim \text{Exp}(\nu), \quad i = 1, 2, \dots
$$

The rate $\nu$ depends on the number of radioactive samples $N$:

$$
\nu = \nu_1 \cdot N
$$

Here, $\nu_1$ is the rate for just one sample.

The total time until the 100th click is:

$$
S = X_1 + X_2 + \cdots + X_{100}
$$

Since each $X_i$ is exponential, their sum $S$ follows a Gamma
distribution:

$$
S \sim \text{Gamma}(100, \text{rate} = \nu)
$$

The system is considered safe if **no more than 100 clicks** happen in
one minute — meaning the 100th click happens **after** 1 minute:

$$
P(S > 1) \geq 0.95
$$

This is the condition we’ll use to estimate the maximum number of
samples $N$ that can be safely stored.

### Theoretical Bound on $N$ Using Markov's Inequality

Let $S := X_1 + X_2 + \cdots + X_{100}$ be the total time until the
100th click.\
Each $X_k \sim \text{Exp}(\nu_1 N)$, so:

$$
\mathbb{E}[S] = \frac{100}{\nu_1 N}
$$

We want the 100th click to happen **after** 1 minute with at least 95%
probability:

$$
P(S > 1) \geq 0.95
$$

Using **Markov’s inequality**: $$
P(S > 1) \leq \frac{\mathbb{E}[S]}{1} = \frac{100}{\nu_1 N}
$$

To ensure $P(S > 1) \geq 0.95$, we require: $$
\frac{100}{\nu_1 N} \geq 0.95 \quad \Rightarrow \quad N \leq \frac{100}{0.95 \cdot \nu_1}
$$

With team ID = 6, we have $\nu_1 = 16$. Therefore: $$
N \leq \frac{100}{0.95 \cdot 16} \approx 6.58
$$

Conclusion: Using Markov’s inequality, the maximum number of radioactive
samples is: $$
\boxed{N \leq 6}
$$

### Theoretical Bound on $N$ Using the Chernoff Bound

We want to ensure the safety condition: $$
P(S \leq 1) \leq 0.05
$$ where
$S = X_1 + \cdots + X_{100} \sim \text{Gamma}(100, \text{rate} = \nu_1 N)$\
and $\nu_1 = 16$, so $\nu = 16N$.

Using the Chernoff bound: $$
P(S \leq a) \leq e^{-sa} \cdot M_S(s)
$$ Set $a = 1$, and choose $s = 1$ for simplicity.

The moment generating function of $S \sim \text{Gamma}(100, \nu)$ is: $$
M_S(s) = \left( \frac{\nu}{\nu - s} \right)^{100} = \left( \frac{16N}{16N - 1} \right)^{100}
$$

The bound becomes: $$
P(S \leq 1) \leq e^{-1} \cdot \left( \frac{16N}{16N - 1} \right)^{100}
$$

We solve the inequality: $$
0.3679 \cdot \left( \frac{16N}{16N - 1} \right)^{100} \leq 0.05
$$

Solving numerically gives: $$
N \leq 4.52 \quad \Rightarrow \quad \boxed{N \leq 4}
$$

Conclusion: Using the Chernoff bound, at most 4 radioactive samples can
be safely stored.

### Theoretical bound on $N$ using the Central Limit Theorem

We consider $$
S := X_1 + X_2 + \cdots + X_{100},
$$ where each interarrival time $X_i$ is exponentially distributed with
parameter $\nu = \nu_1 N$. Thus $X_i \sim \mathrm{Exp}(\nu_1 N)$. The
mean and variance of each $X_i$ are $$
\mathbb{E}[X_i] = \frac{1}{\nu_1 N}
\qquad \text{and} \qquad
\mathrm{Var}(X_i) = \frac{1}{(\nu_1 N)^2}.
$$Since $S$ is a sum of 100 i.i.d. random variables, $$
\mathbb{E}[S] = 100 \cdot \frac{1}{\nu_1 N} = \frac{100}{\nu_1 N},
\qquad
\mathrm{Var}(S) = 100 \cdot \frac{1}{(\nu_1 N)^2}
= \frac{100}{(\nu_1 N)^2},
$$

$$
\sigma_S = \sqrt{\mathrm{Var}(S)}
= \frac{10}{\nu_1 N}.
$$By the Central Limit Theorem, $$
S \approx \mathcal{N}\!\left(\frac{100}{\nu_1 N}, \; \left(\frac{10}{\nu_1 N}\right)^2 \right).
$$The safety condition requires $$
P(S \le 1) \le 0.05.
$$We standardize: $$
Z = \frac{S - \frac{100}{\nu_1 N}}{\frac{10}{\nu_1 N}}
= \frac{\nu_1 N \cdot S - 100}{10}.
$$Thus $$
P(S \le 1)
= P\!\left( Z \le \frac{\nu_1 N \cdot 1 - 100}{10} \right)
\le 0.05.
$$Let $z_{0.05} = -1.645$. Then $$
\frac{\nu_1 N - 100}{10} \le -1.645
\quad \Rightarrow \quad
\nu_1 N \le 100 - 16.45 = 83.55.
$$With $\nu_1 = 16$ (team ID = 6), $$
N \le \frac{83.55}{16} \approx 5.22.
$$

$$
\boxed{N \le 5}
$$

Using the Central Limit Theorem, at most **5 radioactive samples** can
be stored while keeping the probability of exceeding 100 clicks in one
minute below $5\%$.

### Summary of Bounds on $N$

We applied three methods to estimate the maximum number of radioactive
samples $N$ such that $$
P(S \leq 1) \leq 0.05 \quad \text{with} \quad \nu_1 = 16.
$$

-   Using **Markov's inequality**, we get: $N \leq 6$
-   Using the **Chernoff bound**, we get: $N \leq 4$
-   Using the **Central Limit Theorem**, we get: $N \leq 5$

**Conclusion:** The Central Limit Theorem gives the most accurate
estimate.\
The Chernoff bound is a good compromise. The Markov bound is too loose
for practical use.

```{r}

nu_1 <- id + 10      # base rate for one radioactive sample
N <- 5                    # predicted safe number of samples from previous
nu <- nu_1 * N            # final exponential rate parameter


            
x <- rexp(100, rate = nu) # x1, x2, ..., x100 ~ Exp(nu)


S <- sum(x)


cat("Simulated S (sum of 100 interarrival times):", S, "\n")


```

```{r}

K <- 10000                  # number of repetitions 


             
s <- replicate(K, sum(rexp(100, rate = nu)))


p_safe <- mean(s > 1)


cat("Estimated probability of safety (P(S > 1)):", round(p_safe, 4), "\n")


hist(s, breaks = 50, main = "Distribution of total time until 100th click",
     xlab = "Total time S", col = "skyblue", border = "white")
abline(v = 1, col = "red", lwd = 2, lty = 2)
legend("topright", legend = "Threshold S = 1", col = "red", lty = 2, bty = "n")




```

**Conclusion**

We used three theoretical methods (Markov, Chernoff, and the Central
Limit Theorem) to estimate how many radioactive samples can be safely
stored. The CLT gave the most reasonable answer, suggesting that using
up to 5 samples is safe. I confirmed this with simulation, where the
estimated probability of safety (S \> 1) was above 95%. The result
agrees with theory and makes sense based on expectations.

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

2.  Suppose we have a random variable $X$. Explain why
    $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

Because E(X) is linear, while $\frac{1}{X}$ is a non-linear function.
Counterexample:\
Let's take random variable X, such that P(X=1) = 0.5, P(X=2) = 0.5.\
E($\frac{1}{X}$) = 1 \* 0.5 + 0.5 \* 0.5 = 0.75\
$\frac{1}{\mathbb{E(X)}}$ = 1/(1 \* 0.5 + 2 \* 0.5) = 1/1.5 = 2/3\
As we can see, they are not equal\

2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and
    $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations
    $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of
    $Y := \frac{1}{X}$ to calculate the values of
    $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$.
    Comment on the received results;

```{r}
mu <- 6
sigma <- sqrt(2 * 6 + 7)
N <- 100
# Simulating the realization
x_samples <- rnorm(n, mean = mu, sd = sigma)

y_samples <- 1 / x_samples
# Calculating and comparing means
x_bar <- mean(x_samples)

value_1 <- 1 / x_bar

value_2 <- mean(y_samples)

cat("Sample Mean of X:", round(x_bar, 4), "\n")
cat("Value of 1 / X_bar:", round(value_1, 4), "\n")
cat("\n")
cat("Sample Mean of Y:", round(value_2, 4), "\n")
cat("\n")
```

Sample means of Y and 1/X are not equal, because 1/x is not linear, and
is very sensitive to small changes especially around zero.

```         
3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with
    parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the
    Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the
    Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain
    the results. Comment on the difference of relations between the
    pairs of random variables. Which pair of r.v.'s is dependent and
    which one is similar?
```

```{r}
lambda <- 2
X <- rexp(100, rate = lambda)
Y <- rexp(100, rate = lambda)
Z <- log(X) + 5

# QQ plots and scatterplots
par(mfrow = c(2, 2))
qqplot(X, Y, main = "QQ Plot: X vs Y")
abline(0, 1, col = "red")

plot(X, Y, main = "Scatterplot: X vs Y",
     xlab = "X", ylab = "Y", col = "blue")

qqplot(X, Z, main = "QQ Plot: X vs Z")
abline(0, 1, col = "red")

plot(X, Z, main = "Scatterplot: X vs Z",
     xlab = "X", ylab = "Z", col = "blue")
```

$X$ and $Y$ are independent exponential variables, so their QQ and
scatterplots show no clear linear relationship. $Z = \log(X) + 5$ is a
deterministic transformation of $X$, making them perfectly dependent.
The QQ plot and scatterplot of $X$ and $Z$ reveal a clear nonlinear
pattern. Therefore, $(X, Y)$ are independent, while $(X, Z)$ are
dependent.

------------------------------------------------------------------------

2.  You toss a fair coin three times and a random variable $X$ records
    how many times the coin shows Heads. You convince your friend that
    they should play a game with the following payoff: every round
    (equivalent to three coin tosses) will cost £$1$. They will receive
    £$0.5$ for every coin showing Heads. What is the expected value and
    the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

X records how many times the coin shows Heads when a fair coin is tossed
$3$ times. Each toss is an independent Bernoulli trial with probability
$p = 0.5$ of getting a Head. Therefore, X follows a Binomial
distribution: $X \sim \text{Binomial}(n = 3, p = 0.5)$ with pmf
$P(X=k) = \binom{3}{k} \cdot 0.5^k \cdot 0.5^{3-k}$

$E(X) = n p = 3 \cdot 0.5 = 1.5$,\
$\mathrm{Var}(X) = n \cdot p \cdot (1-p) = 3 \cdot 0.5 \cdot 0.5 = 0.75$

$E(Y) = 0.5 \cdot E(X) - 1 = -0.25$,\
$\mathrm{Var}(Y) = (0.5)^2 \cdot \mathrm{Var}(X) = 0.1875$

```         
2.  What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;
```

```{r}
x <- rbinom(n = 100, size = 3, prob = 0.5)

n <- length(x)
mean_manual <- sum(x) / n
var_manual <- sum((x - mean_manual)^2) / (n - 1)

cat("Sample mean  =", mean_manual, "\n")
cat("Sample variance =", var_manual, "\n")

```

The theoretical mean of $X$ is 1.5 and the theoretical variance is
0.75.\
The simulated sample mean ≈ 1.46 and sample variance ≈ 0.61 are
reasonably close to these theoretical values. This small difference is
expected because we only used 100 samples. With a larger sample size,
the Law of Large Numbers predicts that the sample mean and variance
would get even closer to the theoretical values.

```         
3.  What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;
```

```{r}
y <- 0.5 * x - 1
n <- length(y)
mean_y <- sum(y) / n
var_y <- sum((y - mean_y)^2) / (n - 1)

cat("Sample mean of Y =", mean_y, "\n")
cat("Sample variance of Y =", var_y, "\n")
```

The sample mean of $Y$ is approximately $-0.27$, and the sample variance
is around $0.153$. These values are close to the theoretical
expectations: $$
\mathbb{E}[Y] = -0.25 \quad \text{and} \quad \text{Var}(Y) = 0.1875,
$$ which shows that the simulation aligns well with theory. Small
differences are due to random variation in the sample.

------------------------------------------------------------------------

### General summary and conclusions

In this lab, our team completed all tasks related to simulation and
theoretical analysis of random variables in R. We worked with
Exponential, Gamma, and Binomial distributions, calculated expected
values and variances, and applied inequalities (Markov, Chernoff) and
the Central Limit Theorem.

We simulated different scenarios, such as radioactive decay and coin
toss games, and compared simulation results with theoretical values. In
most cases, the simulation outcomes matched well with the theory,
especially for large sample sizes.

Some steps required careful thinking — like interpreting the meaning of
safety conditions or applying the Chernoff bound — but we managed to
understand them by breaking them down. Overall, the lab helped us better
understand how probability theory works in practice.
