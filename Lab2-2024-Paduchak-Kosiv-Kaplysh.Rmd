---
title: 'P&S-2022: Lab assignment 2'
author: 
"Paduchak Marharyta,
Kosiv Marta,
Kaplysh Olha"
output:
  html_document:
    df_print: paged
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
id <- 19

set.seed(id)
p <- id/100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
 
cat("The matrix G is: \n") 
G  
cat("The matrix H is: \n") 
H
cat("The product GH must be zero: \n")
(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages

message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(100)
codewords <- (messages %*% G) %% 2
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
errors <- matrix(
  rbinom(n = nrow(codewords) * ncol(codewords), size = 1, prob = p),
  nrow = nrow(codewords), ncol = ncol(codewords)
)
received <- (codewords + errors) %% 2
```

The next steps include detecting the errors in the received messages, correcting them, and then decoding the obtained messages. After this, you can continue with calculating all the quantities of interest

```{r}
# syndrome (N x 3)
z <- (received %*% H) %% 2

#  the wrong bit no.: z1 + 2*z2 + 4*z3
#  (z1 z2 z3) %*% (1,2,4) = z1*1 + z2*2 + z3*4
pos <- as.vector(z %*% c(1, 2, 4))
corrected <- received
idx <- which(pos > 0)
if (length(idx) > 0) {
  corrected[cbind(idx, pos[idx])] <- 1 - corrected[cbind(idx, pos[idx])]
}

#  drop = FALSE to stay matrix not vector
decoded <- corrected[, c(3, 5, 6, 7), drop = FALSE]

success <- rowSums(decoded != messages) == 0

p_hat <- mean(success)

cat("Estimated probability of correct transmission p_hat =", p_hat, "\n")
```

The estimator $\hat p$ is the sample mean of the indicator of success and therefore a consistent estimator of the true probability $p^*$. According to the Law of Large Numbers (LLN), as the number of simulations $N$ increases, the sample mean $\hat p$ converges to the true probability $p^*$.

```{r}
p_hat <- mean(success)
N     <- length(success)

SE    <- sqrt(p_hat * (1 - p_hat) / N)
# z0.975 ≈ 1.96
eps   <- 1.96 * SE
CI    <- c(p_hat - eps, p_hat + eps)

cat("SE =", SE, "\n95% CI = (", CI[1], ",", CI[2], ")\n")
```

**What choice of** $N$ guarantees that $\varepsilon \le 0.03$?

Since the confidence interval has the general form $(\hat p-\varepsilon, \hat p + \varepsilon)$ its half–length $\varepsilon$ is given by $\varepsilon = z_{0.975} * SE = 1.96 * \sqrt(\hat p * (1 - \hat p) / N)$.

From that: 1.96 $* \sqrt(\frac{\hat p * (1 - \hat p)} N) \le$ 0.03

we have: $N \ge 1.96^2 * \frac{\hat p * (1 - \hat p)}{0.03^2}$

The function $p(1 - p)$ reaches its maximum at $p = 0.5$, and its maximum value is $0.5 * 0.5 = 0.25$. Therefore, $p(1 - p) \le 0.25$ for all $p$ from integral $[0,1]$. This bound is used to obtain a worst–case estimate for $N$. So we have $N \ge 1068$.

But if we want to use $\hat p$ that we previously calculated, then:

```{r}

N_ <- ceiling((1.96^2 * p_hat * (1 - p_hat)) / (0.03^2))
cat("N >= ",N_)

```

```{r}
## 1) Гістограма для "4-бітне повідомлення без кодування без Hamming-коду, тобто без захисту від помилок."
errors_4   <- matrix(rbinom(n = nrow(messages)*4, size = 1, prob = p),
                     nrow = nrow(messages), ncol = 4)
received_4 <- (messages + errors_4) %% 2
k_uncoded  <- rowSums(received_4 != messages)  # k = 0..4

hist(k_uncoded, breaks = seq(-0.5, 4.5, by = 1),
     main = "Number of errors in a 4-bit message (uncoded)",
     xlab = "k errors", ylab = "Frequency")

cat("Uncoded counts:\n"); print(table(k_uncoded))

# (опційно) теоретичний Binomial(4, p) для порівняння
cat("Theoretical Binomial(4, p):\n"); print(dbinom(0:4, size = 4, prob = p))


## 2) Гістограма для "після Hamming-декодування" 
k_decoded <- rowSums(decoded != messages)  # k = 0..4 помилок у відновлених 4 бітах
hist(k_decoded, breaks = seq(-0.5, 4.5, by = 1),
     main = "Number of wrong data bits after Hamming decoding",
     xlab = "k wrong data bits", ylab = "Frequency")

cat("Decoded counts:\n"); print(table(k_decoded))
```

For the uncoded transmission, the number of bit errors $k = {0,1,2,3,4}$ in a 4-bit message follows a Binomial(4, p) distribution, since each bit is independently flipped with probability $p$. This is clearly reflected in the empirical results which are in good agreement with the theoretical Binomial probabilities.

However, after Hamming decoding, the distribution of the number of remaining wrong data bits is no longer binomial. The decoder corrects all single-bit errors and changes the probability structure of multi-bit errors, which breaks the independence assumption required for a Binomial model. As a result, the decoded histogram becomes heavily concentrated at $k = 0$, showing that the Hamming code successfully reduces the number of errors but produces a distribution that is no longer one of the standard known distributions such as Binomial.

**Summary** In this task, we investigated the reliability of the $[7,4]$ Hamming code by simulating the full process of encoding, transmission through a binary symmetric channel, and decoding. The empirical estimate $\hat p$ of the probability $p^*$ of correct transmission was obtained by repeating the experiment $N$ times. By the Law of Large Numbers, $\hat p$ converges to $p^*$ for large $N$, which explains why our estimate becomes more accurate as the sample size increases.

Using the Central Limit Theorem and the standard error of the Bernoulli success indicator, we constructed a 95% confidence interval for $p^*$ of form $(\hat p-\varepsilon, \hat p + \varepsilon)$. We then determined that choosing $N \ge 1068$ (in a worst–case) guarantees a half–length $\varepsilon \le 0.03$. Finally, we compared the distribution of the number of errors in a 4-bit message before and after decoding. The uncoded case follows a Binomial(4, p) distribution, while the decoded case does not correspond to a standard distribution due to the error–correcting mechanism of the Hamming code.

Overall, our results confirm that the Hamming $[7,4]$ code significantly reduces the number of errors and reliably corrects single–bit corruptions, demonstrating a clear improvement in transmission accuracy.

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

```{r}

team_id = 5

T_years <- 30.1
T_sec <- T_years * 365.25 * 24 * 3600
lambda <- log(2) / T_sec

N_A <- 6e23
M <- 137
m <- team_id * 1e-6
N <- m * N_A / M
mu <- N * lambda
mu
K <- 1e3
n <- 100
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))

```

We first calculate the Poisson parameter mu for one sample of Cesium-137 and simulate n independent samples K times. Each column of the matrix represents one repetition of n samples, and colMeans gives the sample mean of decays for each repetition. This will allow us to observe how the sample means are distributed.

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu_normal <- mu
sigma_normal <- sqrt(mu / n)
```

#### We can now plot ecdf and cdf

```{r}
compare_ecdf_normal <- function(n, mu, K) {
  sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
  
  mu_normal <- mu
  sigma_normal <- sqrt(mu / n)
  
  xlims <- c(mu_normal-3*sigma_normal, mu_normal+3*sigma_normal)
  Fs <- ecdf(sample_means)
  
  plot(Fs, 
       xlim = xlims, 
       ylim = c(0,1),
       col = "blue",
       lwd = 2,
       main = paste("n =", n, "- ECDF vs Normal CDF"))
  curve(pnorm(x, mean = mu_normal, sd = sigma_normal), col = "red", lwd = 2, add = TRUE)
  
  x_vals <- sort(sample_means)
  cdf_vals <- pnorm(x_vals, mean = mu_normal, sd = sigma_normal)
  diffs <- abs(Fs(x_vals) - cdf_vals)
  max_diff <- max(diffs)
  
  cat("n =", n, "-> max difference between ECDF and Normal CDF:", max_diff, "\n")
}

compare_ecdf_normal(1000, mu, K)
```

This function visualizes the empirical cumulative distribution function (ECDF) of sample means alongside the corresponding normal CDF. The maximal difference (max_diff) quantifies the discrepancy and helps illustrate how well the normal approximation works for different n.

```{r}
compare_ecdf_normal(5, mu, K)
compare_ecdf_normal(10, mu, K)
compare_ecdf_normal(50, mu, K)
```

$$
\Pr(S_n \ge a) \le \frac{\mathbb{E}[S_n]}{a} = \frac{n \mu}{a} \le 0,05
$$

```{r}
a <- 8e8

n_markov <- max(floor(0.05 * a / mu), 1)
cat("Markov bound on n:", n_markov, "\n")
```

$$
\Pr(S_n \ge (1+\delta) n \mu) \le \left(\frac{e^\delta}{(1+\delta)^{1+\delta}}\right)^{n \mu}, 
\quad \delta > 0
$$

$$
\text{Choose } \delta \text{ s.t. } (1+\delta)n\mu = a \implies \delta = \frac{a}{n\mu}-1
$$

```{r}
chernoff_max_n_simple <- function(mu, a, p_target=0.95){
  log_threshold <- log(1 - p_target)
  n_max <- floor(a / mu)
  n <- n_max
  while(n > 0){
    delta <- a / (n*mu) - 1
    if(delta <= 0){
      n <- n - 1
      next
    }
    log_p <- n*mu*(delta - (1+delta)*log(1+delta))
    if(log_p <= log_threshold) break
    n <- n - 1
  }
  return(n)
}

n_chernoff <- chernoff_max_n_simple(mu, a)
cat("Chernoff bound on n:", n_chernoff, "\n")

```

$$
\Pr(S_n < a) \approx \Phi\Bigg(\frac{a - n \mu}{\sqrt{n \mu}}\Bigg) \ge 0.95
$$

$$
\Phi^{-1}(0.95) = z_{0.95} = 1.645 \implies \frac{a - n \mu}{\sqrt{n \mu}} \ge 1.645
$$

Value of $z_{0.95}$ was taken from the table

$$
a - n \mu \ge 1.645 \sqrt{n \mu} \implies n \text{ can be solved numerically.}
$$

```{r}

z <- qnorm(0.95)
f <- function(n) { a - n*mu - z*sqrt(n*mu) }
n_clt <- floor(uniroot(f, lower = 1, upper = 1e9)$root)
cat("CLT bound on n:", n_clt, "\n")

```

```{r}

cat("Markov bound on n:", n_markov, "\n")
cat("Chernoff bound on n:", n_chernoff, "\n")
cat("CLT bound on n:", n_clt, "\n")

```

Markov and CLT can be very different numerically, especially when the expected sum is huge and the threshold is in an extreme tail.

Chernoff is typically the most reliable for upper-tail bounds.

```{r}

n <- n_chernoff
sim_matrix <- matrix(rpois(n*K, lambda = mu), nrow = n)
sum_decays <- colSums(sim_matrix)
num_below <- sum(sum_decays < a)
empirical_prob <- num_below / K

cat("n =", n, "\n")
cat("Empirical probability:", empirical_prob, "\n")
```

In this task, we modeled the radioactive decay of Cesium-137 samples using the Poisson distribution. We calculated the Poisson parameter $\mu$ for a single sample and simulated $n$ independent samples $K$ times. The empirical cumulative distribution function (ECDF) of the sample means closely matches the corresponding normal distribution as $n$ increases, illustrating the central limit theorem in practice.

We then computed theoretical bounds on the maximum number of samples $n$ such that the total number of decays in one second remains below $8 \times 10^8$ with at least 95% probability. The Markov inequality gives a very conservative estimate, the Chernoff bound provides a tight and reliable upper limit, and the CLT approximation is reasonably accurate for large $n$. Simulation results confirm that the Chernoff-based $n$ indeed achieves the desired probability level, demonstrating the practical usefulness of probabilistic inequalities and the normal approximation in extreme-event estimation.

Overall, the Chernoff bound is the most suitable method for controlling rare, high-deviation events in this context, while the normal approximation is very effective for understanding the distribution of sample means.

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

#### First, generate samples an sample means:

```{r}
nu1 <- 1  # change this!
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- 0       # change this!
sigma <- 1    # change this!
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

**Next, proceed with all the remaining steps**

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:\`**

1.  **In this part, we discuss independence of random variables and its moments: expectation and variance.**

\$\$ 1. Suppose we have a random variable $X$. Explain why $\mathbb{E}\!\left(\frac{1}{X}\right) \neq \frac{1}{\mathbb{E}(X)}$.

For a discrete random variable, $$
\mathbb{E}\!\left(\frac{1}{X}\right) 
= \sum_{i} \frac{1}{x_i} \, p_X(x_i)
= \frac{p_X(x_1)}{x_1} + \cdots + \frac{p_X(x_n)}{x_n},
$$ while $$
\frac{1}{\mathbb{E}(X)} 
= \frac{1}{\sum_{i} x_i p_X(x_i)}
= \frac{1}{p_X(x_1)x_1 + \cdots + p_X(x_n)x_n}.
$$ These two expressions involve fundamentally different algebraic operations: the first is a weighted sum of reciprocals, while the second is the reciprocal of a weighted sum. In general, $$
\frac{p_X(x_1)}{x_1} + \cdots + \frac{p_X(x_n)}{x_n}
\ne 
\frac{1}{p_X(x_1)x_1 + \cdots + p_X(x_n)x_n},
$$ so $$
\mathbb{E}\!\left(\frac{1}{X}\right) \neq \frac{1}{\mathbb{E}(X)}.
$$

For a continuous random variable, a similar reasoning applies: $$
\mathbb{E}\!\left[\frac{1}{X}\right]
= \int_{-\infty}^{\infty} \frac{1}{x} f(x)\,dx,
\qquad
\frac{1}{\mathbb{E}[X]} 
= \frac{1}{\int_{-\infty}^{\infty} x f(x)\,dx}.
$$ Again, $$
\int \frac{1}{x} f(x)\,dx 
\neq 
\frac{1}{\int x f(x)\,dx},
$$ in general, so the equality does not hold.

2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of $Y := \frac{1}{X}$ to calculate the values of $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$. Comment on the received results;

```{r}
teamidnumber = 19
mu <- teamidnumber
sigma <- sqrt(2 * teamidnumber + 7)
n <- 1e5

x <- rnorm(n, mean = mu, sd = sigma)
y <- 1 / x

# Compute statistics
one_over_mean_x <- 1 / mean(x)
mean_y <- mean(y)

cat("1 / mean(X) =", one_over_mean_x, "\n")
cat("mean(1/X)  =", mean_y, "\n")
```

Then, taking the average first and then the reciprocal is not the same as taking the reciprocal first and then averaging. The simulation confirms that these two values are generally different.

3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?

```{r}
lambda <- 2
x <- rexp(n, rate=lambda)
y <- rexp(n, rate=lambda)
z <- log(x) + 5
```

```{r}
qqplot(x, y,
       main = "Quantile-Quantile: X, Y",
       xlab = "Quantiles of X",
       ylab = "Quantiles of Y",
       pch = 19, col = "blue")
abline(0, 1, col = "red", lwd = 2)
legend("topleft",
       legend = "y = x",
       col = "red",
       lwd = 2,
       bty = "n")

qqplot(x, z,
       main = "Quantile-Quantile Plot: X, Z",
       xlab = "Quantiles of X",
       ylab = "Quantiles of Z",
       pch = 19, col = "blue")
abline(0, 1, col = "red", lwd = 2)
legend("topleft",
       legend = "y = x",
       col = "red",
       lwd = 2,
       bty = "n")
```

The Quantile-Quantile plot compares the quantiles of two random variables. Points near $y = x$ indicate similar distributions.

For $(X, Y)$, the points lie close to $y = x$, showing that $X$ and $Y$ have similar distributions (both exponential with the same parameter).

For $(X, Z)$, the points follow $z = \log(x) + 5$, reflecting $Z = \log(X) + 5$. Hence, $X$ and $Z$ are not similar, as their distributions differ.

```{r}
plot(x, y, type = "p", 
     main = "Scatter Plot of X and Y",
     xlab = "X",
     ylab = "Y",
     col = "blue", pch = 19)


plot(x, z, type = "p", 
     main = "Scatter Plot of X and Z",
     xlab = "X",
     ylab = "Z",
     col = "blue", pch = 19)
curve(log(x) + 5, from = 0.1, to = 10, add = TRUE, col = "red", lwd = 2)
legend("topleft", 
       legend = "z = log(x) + 5",
       col = "red", 
       lwd = 2,
       bty = "n")
```

**Conclusion:**

In this task, we explored several fundamental concepts in probability and statistics related to expectation, independence, and relationships between random variables.

First, we demonstrated that the expectation operator is not invertible in general, i.e. $$
\mathbb{E}\!\left(\frac{1}{X}\right) \neq \frac{1}{\mathbb{E}(X)}.
$$ This follows because taking the reciprocal is a nonlinear transformation, and the linearity property of expectation does not apply. Both analytical reasoning and simulation confirmed that the two quantities differ.

Next, using normally distributed random variables, we verified numerically that $$
\frac{1}{\overline{X}} \neq \overline{\left(\frac{1}{X}\right)},
$$ again illustrating the effect of nonlinearity in the reciprocal transformation.

Finally, we examined dependence and similarity of distributions. The exponentially distributed random variables $X$ and $Y$ (with the same rate parameter $\lambda = 2$) were shown to be independent but identically distributed, as indicated by both scatter plots and Q–Q plots lying close to the line $y = x$.\
In contrast, the pair $(X, Z)$, where $Z = \log(X) + 5$, displayed a deterministic nonlinear relationship—evident from the curved scatter pattern and deviation from $y = x$ in the Q–Q plot. This shows that $X$ and $Z$ are dependent but not similarly distributed.

Overall, the experiments confirm that nonlinear transformations alter the distribution and expectation of random variables, and that Q–Q and scatter plots are effective tools for visualizing dependence and distributional similarity.

```         
------------------------------------------------------------------------
```

2.  You toss a fair coin three times and a random variable $X$ records how many times the coin shows Heads. You convince your friend that they should play a game with the following payoff: every round (equivalent to three coin tosses) will cost £$1$. They will receive £$0.5$ for every coin showing Heads. What is the expected value and the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

    2.  What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;

        ```{r}
        # YOUR CODE HERE
        ```

    3.  What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;

        ```{r}
        # YOUR CODE HERE
        ```

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### General summary and conclusions

Summarize here what you've done, whether you solved the tasks, what difficulties you had etc.

In the **second task** we modelled the radioactive decay of Cesium-137 samples using the Poisson distribution. The Poisson parameter $\mu$ was computed from the physical constants of the isotope, and simulations of $n$ independent samples confirmed that the sample means follow a distribution increasingly close to $\mathscr{N}(\mu, \mu/n)$ as $n$ grows, illustrating the Central Limit Theorem.

We then determined the largest number of samples $n$ such that the total number of decays in one second remains below $8 \times 10^8$ with probability at least $0.95$. Theoretical estimates based on the Markov inequality, Chernoff bound, and Central Limit Theorem were compared: the Markov bound proved overly conservative, the CLT gave a reasonable approximation for large $n$, and the

In the **fourth task part one** we examined key properties of random variables, focusing on expectation, nonlinearity, and dependence. We first showed analytically and through simulation that $\mathbb{E}\!\left(\frac{1}{X}\right) \neq \frac{1}{\mathbb{E}(X)}$, since the reciprocal is a nonlinear transformation and the linearity of expectation does not apply. Using normal samples, we confirmed numerically that $\frac{1}{\overline{X}} \neq \overline{\left(\frac{1}{X}\right)}$.

Next, we analyzed dependence between random variables. Independent exponential variables $X$ and $Y$ (with the same parameter $\lambda = 2$) exhibited similar distributions, as seen from Quantile-Quantile and scatter plots lying along $y = x$. In contrast, $Z = \log(X) + 5$ was deterministically related to $X$, producing a nonlinear scatter and a distinct Quantile-Quantile curve, indicating dependence and dissimilar distributions.

Overall, this task illustrated how nonlinear transformations affect expectations and distributions, and how graphical tools like Quantile-Quantile and scatter plots reveal dependence and similarity between random variables.
